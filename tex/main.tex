% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 %reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage{dsfont}
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\newcommand{\given}[2]{p( #1 | #2 )}
\newcommand{\ppop}[0]{p_{\text{pop}}}
\newcommand{\pdet}[0]{p_{\text{det}}}
\newcommand{\ndet}[0]{N_{\text{det}}}
\newcommand{\nobs}[0]{N_{\text{obs}}}
\newcommand{\npix}[0]{N_{\text{pix}}}
\newcommand{\ngal}[0]{N_{\text{gal}}}
\newcommand{\dgw}[0]{\vec{d}_{\text{GW}}}
\newcommand{\dem}[0]{\vec{d}_{\text{EM}}}
\newcommand{\ind}[1]{\mathds{1}_{\{ #1 \}}}

\begin{document}

\preprint{APS/123-QED}

%\title{Manuscript Title:\\with Forced Linebreak}% Force line breaks with \\
\title{Cosmology with standard sirens}
%\thanks{A footnote to the article title}%

\author{Bernardo Porto Veronese}
%\altaffiliation[Also at ]{PPGCOSMO, UFES}%Lines break automatically or can be forced with \\
\email{bernardo.veronese@edu.ufes.br}
\affiliation{%
	PPGCOSMO, UFES
}%

%\collaboration{MUSO Collaboration}%\noaffiliation

\date{\today}% It is always \today, today,
%  but any date may be explicitly specified

\begin{abstract}
	An article usually includes an abstract, a concise summary of the work
	covered at length in the main body of the article.
	\begin{description}
		\item[Usage] Secondary publications and information retrieval purposes.
		\item[Structure] You may use the \texttt{description} environment to structure your abstract; use the
		      optional argument of the \verb+\item+ command to give the category of each item.
	\end{description}
\end{abstract}

%\keywords{Suggested keywords}%Use showkeys class option if keyword
%display desired
\maketitle

%\tableofcontents

\section{\label{sec:introduction}Introduction} The idea of using gravitational waves (GWs) from compact binary mergers to measure
cosmological parameters was first introduced by Bernard Schutz in 1986~\cite{Schutz:1986gp}. These
signals directly provide a measurement of the luminosity distance measurement to the source, which
is therefore independent of the cosmic distance ladder. With the addition of redshift information,
measurements can therefore be made of those cosmological parameters which impact the expansion
history of the Universe, such as the Hubble constant ($H_0$). This approach is independent of all
other local measurements to date.

The standard siren method probes the expansion history of the universe with the distance-redshift
relation, with which one can infer the cosmological parameters such as $H_0$ and the dark energy
equation of state parameter $w$:~\cite{Hogg:1999ad}

\begin{align}
	\label{eq:intro:distance-redshift-relation}
	D_l(z)           & = (1+z)\frac{c}{H_0 \sqrt{\Omega_{K}}} \sinh \left[ \sqrt{\Omega_{K}} \int_0^z \frac{H_0}{H(z') dz'}\right] \\
	\nonumber
	\frac{H(z)}{H_0} & = \sqrt{\Omega_m (1+z)^3 + \Omega_K (1+z)^2 + \Omega_{de} (1+z)^{3(1+w)}}.
\end{align}

To lighten notation, we have omitted the 0-subscript next to the $\Omega_i$'s, although they
correspond to the present day values in the above equation. Note that using
Eq.~\eqref{eq:intro:distance-redshift-relation} requires specifying a cosmological model.

The accuracy of the GW luminosity distance measurement is typically of the order of 10\%. The main
source of uncertainty comes from the degeneracy between the distance and inclination angle of the
source. The latter is defined as the angle between the line-of-sight vector from the source to the
detector and the orbital-angular momentum of the binary system.

From the GW data, it is possible to infer the luminosity distance to the binary source, but not the
redshift, as the latter is degenerate with the chirp mass in the GW waveform modelling. It is
therefore necessary to complement the data with another source of information that provides the
redshift measurement. Multi-messenger observations, such as neutron star mergers with
electromagnetic counterparts like short gamma-ray bursts or kilonovae, provide the most
straight-forward measurement \cite{Holz:2005df,Dalal:2006qt}. An electromagnetic counterpart like a
kilonova can typically be pinpointed to a specific galaxy, thereby identifying the host galaxy of
the GW merger. The GW signal provides the distance to the host galaxy, while its electromagnetic
spectrum provides the redshift. These sources are typically referred to as bright sirens. So far,
the only confirmed such event has been the binary neutron star detection GW170817, which occurred
so exceptionally close to our galaxy - at $d \sim 40 \,$Mpc - that a direct, model-independent
estimation of $H_0$ with Hubble's law,

\begin{equation}
	v_H = H_0 d,
\end{equation}

could be made by measuring the Hubble flow velocity $v_H$, resulting in $H_0 = 70.0^{+12.0}_{-8.0}$
km s$^{-1}$ Mpc$^{-1}$~\cite{LIGOScientific:2017adf}.

As stated above, almost all GW events have been detected without an EM counterpart. These
\textit{dark sirens} can be used to probe the expansion of the universe provided that they are
complemented with an external redshift measurement. In his original paper, Schutz suggested that
this information could be inferred from galaxy catalogs: each galaxy's redshift contributes to a
hypothetical estimation of $H_0$, such that the galaxy structure within a GW event's localisation
volume is reflected in the $H_0$ posterior it produces. How informative the individual events are
will depend strongly on their localisation volumes. By combining the contributions of many events,
the true value of $H_0$ will be measured as other values will statistically average out. Such
analyses have been carried out in the literature,
see~\cite{DelPozzo:2011vcw,Chen:2017rfc,LIGOScientific:2018gmd,Gray:2019ksv,DES:2019ccw}. As an
example, Ref.~\onlinecite{DES:2020nay} applied the galaxy catalog method with the two best
localized dark sirens, GW170814 and GW190814, and the photo-$z$ catalog from the Dark Energy Survey
(DES)~\cite{thedarkenergysurveycollaboration2005dark}. A joint analysis with the bright siren event
GW170817 provided an $\sim 18\%$ improvement on the 68\% confidence interval compared to inferring
$H_0$ with GW170817 alone. Another study used 8 well-localised dark sirens alone to infer $H_0 =
	79.8^{+19.1}_{-12.8}$~\cite{Palmese_2023}.

At higher redshifts, galaxy-wide surveys are incomplete, and the probability that the catalog
contains the merger's host galaxy decreases. On the other hand, both the the gravitational wave
sources and galaxies are tracers of the matter density, and therefore, they are spatially
correlated through the underlying matter field. Therefore, if the events are well localized,
angular correlations between galaxy distributions in redshift and merger distributions in
luminosity distance may be used to infer cosmological parameters. Some authors have explored this
idea with forecasts for 3G detectors~\cite{Oguri_2016}. Refs.~\onlinecite{Bera_2020,Mukherjee_2021}
used simulated data to analyse the method's constraining power on $H_0$ for different numbers of
events. Their results show that the Hubble constant can be measured with $~ 2.5\%$ accuracy for
$\mathcal{O}$(100) events. Finally, this technique is not exclusive to standard sirens, and can be
applied to any redshift-free distance tracer, such as type Ia
supernovae~\cite{mukherjee2018classical}.

Alternative methods have been explored in the literature where the inference was set up using GW
data alone. One such method consists of using the prior knowledge of the star formation rate and
time delay distribution of binary mergers for modelling the prior on the merger population
distribution~\cite{Ding_2019, Ye_2021, Leandro_2022}. In order to avoid biases by using a fixed
merger distribution, it is important to jointly fit its hyperparameters with the GW data.

The different methods presented above are not at all independent. Galaxy catalogs, for instance,
are most effective with well-localised, low-redshift events. Such events need to be marginalized
over a smaller number of galaxies than those that encompass large volumes so that they are more
likely to provide better constraints. Better localized events also tend to be less distant, which
means that a standard siren analysis would mostly be sensitive to the Hubble constant rather than
to other cosmological parameters. At higher redshifts, the catalogs become increasingly
uninformative due to their incompleteness.

This manuscript is organized as follows. In Sec.~\ref{sec:framework}, we will go over the
statistical formalism for data analysis with standard sirens, and we will specialize in the galaxy
catalog method.

\section{\label{sec:framework}Statistical framework}

In gravitational-wave astronomy, one subject of interest is extracting the distributional
properties of a population of sources based on a set of observations which are drawn from that
distribution. Any methodology that leads to unbiased estimates of the population parameters must
simultaneously account for measurement uncertainties and selection effects. One way with which the
latter affects the observed population is a Mamquist bias: the loudest or brightest sources are
more likely to be detected. The standard formalism for extracting the true source population
parameters by incorporating these biases in the analysis is frequently labeled as Hierarchical
Bayesian inference, see~\cite{Loredo:2004nn,Mandel:2018mve,Vitale_2021}.

In the discussion below, we will follow the framework outlined in Ref.~\onlinecite{Gair_2023},
which is a pedagogical resource on the galaxy catalog approach.

The GW population distribution is sampled with a set of $\nobs$ \textit{observed} events with true
parameters $\{ \vec{\theta}_i \}$, $i \in \{1, \cdots, \nobs\}$. We do not have direct access to
the true parameters because of noise; instead, we have a set of measured data $\{ \vec{d}_i \}$.
The $\vec{\theta}_i$ are the individual object parameters, although we are interested in the
population hyperparameters, which we call $\vec{\lambda}$. We cannot determine $\vec{\lambda}$
directly, but we can compute the posterior probability given the observations. In the usual
Bayesian formalism,

\begin{equation}
	\given{\vec{\lambda}}{\{\vec{d}_i \}} =
	\frac{\given{\{\vec{d}_i \}}{\vec{\lambda}} \pi(\vec{\lambda})}{p(\{\vec{d}_i \})}
\end{equation}

where $\given{\{\vec{d}_i \}}{\vec{\lambda}}$ is the likelihood of observing the dataset given the
population properties, $\pi(\vec{\lambda})$ is the prior on $\vec{\lambda}$ and $p(\{\vec{d}_i \})$
is the evidence, which is the integral of the numerator over $\vec{\lambda}$.

In the spirit of Ref.~\onlinecite{Mandel:2018mve}, we first start with the idealized scenario where
the event parameters are perfectly measured. The total likelihood for the set of $\nobs$
independent measurements is then

\begin{equation}
	\label{eq:stat:posterior-no-noise-no-bias}
	\given{\{ \vec{\theta}_i \}}{\vec{\lambda}} =
	\prod_{i=1}^{\nobs} \frac{\ppop(\vec{\theta}_i | \vec{\lambda})}{\int \ppop(\vec{\theta}_i | \vec{\lambda}) d\vec{\lambda}}
\end{equation}

where $\ppop(\vec{\theta} | \vec{\lambda})$ is related to the number density $dN$ of objects
expected to be found in the region $[\vec{\theta}, \vec{\theta} + d\vec{\theta}]$:

\begin{equation}
	\label{eq:stat:ppop}
	dN = N \ppop(\vec{\theta} | \vec{\lambda}) d\vec{\theta}
\end{equation}

We shall build an incrementally more robust model than
Eq.~\eqref{eq:stat:posterior-no-noise-no-bias}. Let us first consider the presence of selection
effects: not all events are equally likely to be detected. We can encode this with a detection
probability $\pdet$. In the perfect measurement idealization, this detection probability becomes a
function of the parameters $\vec{\theta}$ only. In the general case, where noise is present, the
detection probability is a function of the data. Let $\mathcal{D}$ be the set of all data. To
determine whether an event is detectable, one can use a detection statistic $\rho_{\mathcal{D}}$,
which can be calculated for each piece of data. In practice, this statistic can be the
signal-to-noise ratio (SNR), the false-alarm rate, etc. We split $\mathcal{D}$ into two disjoints
sets, $\mathcal{D}_<$ and $\mathcal{D}_\geq$, according to whether $\rho_\mathcal{D}$ is smaller
than a threshold $\rho_{\text{tr}}$ or not. Then

\begin{equation}
	\label{eq:stat:detection-prob}
	\pdet(\vec{\theta}) = \int_{\mathcal{D}_\geq} \given{\vec{d}}{\vec{\theta}}d\vec{d}
\end{equation}

The probability of observing a particular dataset $\vec{d}$ given the assumed population
distribution parameterised by $\vec{\lambda}$ is

\begin{equation}
	\given{\vec{d}}{\vec{\lambda}} =
	\frac{\int \given{\vec{d}}{\vec{\theta}} \ppop(\vec{\theta} | \vec{\lambda} ) d\vec{\theta}}{\alpha(\vec{\lambda})}
\end{equation}

where $\alpha(\vec{\lambda})$ is a normalization factor integrated over the set of detectable data,

\begin{align}
	\alpha(\vec{\lambda}) & =
	\int_{\mathcal{D}_\geq} d\vec{d} \int \given{\vec{d}}{\vec{\theta}} \ppop(\vec{d} | \vec{\lambda} ) d\vec{\theta}                                           \\
	                      & = \int \left[ \int_{\mathcal{D}_\geq} \given{\vec{d}}{\vec{\theta}} d\vec{d} \right]  \ppop(\vec{d} | \vec{\lambda} ) d\vec{\theta} \\
	\label{eq:stat:nalpha}
	                      & = \int \pdet(\vec{\theta}) \ppop(\vec{\theta} | \vec{\lambda} ) d\vec{\theta}
\end{align}

Hence, in the presence of both measurement uncertainties and selection effects,
Eq.~\eqref{eq:stat:posterior-no-noise-no-bias} becomes

\begin{equation}
	\given{\{ \vec{d}_i \}}{\vec{\lambda}} =
	\prod_{i=1}^{\nobs}
	\frac{\int \given{\vec{d_i}}{\vec{\theta}} \ppop(\vec{\theta} | \vec{\lambda} ) d\vec{\theta}}{\int \pdet(\vec{\theta}) \ppop(\vec{\theta} | \vec{\lambda} ) d\vec{\theta}}
\end{equation}

We can also include the population rate into the framework. The probability of observing $k$ events
with an expected number of detections $\ndet$ is given by a Poisson distribution as

\begin{equation}
	\label{eq:stat:poisson}
	\given{k}{\ndet} = e^{-\ndet}(\ndet)^{\nobs}
\end{equation}

The usual $\nobs !$ term is absent in Eq.~\eqref{eq:stat:poisson} because the events are
distinguishable from the observed data. When accouting for selection effects, the expected number
of detections $\ndet$ becomes

\begin{align}
	\ndet(\vec{\lambda}) & = \int_{\mathcal{D}_\geq}  d\vec{d} \int \given{\vec{d}}{\vec{\theta}} \frac{dN}{d\vec{\theta}} d\vec{\theta} \\
	                     & = \int \left[ \int_{\mathcal{D}_\geq} \given{\vec{d}} d\vec{d} \right]\frac{dN}{d\vec{\theta}} d\vec{\theta}  \\
	                     & = \int \pdet(\vec{\theta})\frac{dN}{d\vec{\theta}} d\vec{\theta}                                              \\
	                     & = \int \pdet(\vec{\theta}) N \ppop(\vec{\theta} | \vec{\lambda}) d\vec{\theta}                                \\
	                     & = N \alpha(\lambda)
\end{align}

where the last two equalities are derived from Eq.~\eqref{eq:stat:ppop} and
Eq.~\eqref{eq:stat:nalpha} respectively. The full posterior with the population rate is then

\begin{align}
	\given{\vec{\lambda}, N }{\vec{d}} & = \given{N}{\vec{\lambda}, \vec{d}}\given{\vec{\lambda}}{\vec{d}}                    \\
	                                   & = e^{-\ndet}(\ndet)^{\nobs} \pi(N) \pi(\vec{\lambda}) \alpha(\vec{\lambda})^{-\nobs}
	\prod_{i=1}^{\nobs} \int \given{\vec{d_i}}{\vec{\theta}} \ppop(\vec{\theta} | \vec{\lambda} ) d\vec{\theta}
\end{align}

If a prior $\pi(N) \propto 1/N$ is assumed on the population rate, then the posterior can be
marginalized over $N$:

\begin{align}
	\int e^{-\ndet}(\ndet)^{\nobs} \frac{dN}{N} & = \int e^{-\ndet}(\ndet)^{\nobs - 1} d\ndet \\
	                                            & = (\nobs - 1)!
\end{align}

So far, the framework we developed has been general; we now specify to the gravitational wave case.
The individual event parameters $\vec{\theta}$ describe the compact binary coalescence (CBC): the
individual masses and spins, sky position, polarization, inclination angle, luminosity distance,
and redshift. The population parameters $\vec{\lambda}$ can be split into three groups: mass, rate
and cosmological parameters. The mass parameters specify the GW mass model, such as minimum and
maximum mass, slopes, the positions of any features in the mass distribution function, etc. These
are used in the spectral siren method. The rate parameters are used in the model to describe how
the CBC merger rate evolves with redshift. Finally, the cosmological parameters are the constants
which appear in Eq.~\eqref{eq:intro:distance-redshift-relation}, namely $\{H_0, \Omega_m,
	\Omega_{de}, w\}$.

\subsection{A simplified approach}

In this section, we reproduce the formalism developed in Ref.~\onlinecite{Gair_2023}. In that
paper, the authors perform a mock data analysis of the dark siren approach using the galaxy catalog
method to demonstrate its capability to recover an unbiased posterior for $H_0$. They consider that
the remaining cosmological parameters, such as $\Omega_m$ and $w_\text{DE}$, are fixed to fiducial
values. They make a series of simplifying assumptions on $p_{det}$, $p_{pop}$ and
$\given{\vec{d}}{\vec{\theta}}$, which we discuss below.

To account for selection effects, they neglect the effects of the (sky-dependent) GW detector
sensitvity and detector-frame mass. Instead, detection is assumed to happen if the
\textit{measured} luminosity distance is smaller than a threshold, $d_L^{\rm{th}}$. If the GW
likelihood is taken to be gaussian, that is,

\begin{equation}
	\mathcal{L}(\hat{d}_L^i | d_L(z, H_0))
	= \frac{1}{\sqrt{2 \pi} \sigma_{d_L}} \exp{\left [-\frac{1}{2} \left (\frac{\hat{d}_L^i - d_L(z, H_0)}{\sigma_{d_L}} \right )^2 \right ]},
\end{equation}
then the detection probability can be expressed analytically with Eq.~\eqref{eq:stat:detection-prob}:

\begin{align}
	\nonumber
	\pdet(z, H_0) & = \int_{-\infty}^{d_L^{\rm{th}}} \mathcal{L}(\hat{d}_L^i | d_L(z, H_0)) d \hat{d}_L^i                                    \\
	              & = \frac{1}{2} \left [ 1 + \text{erf} \left (\frac{\hat{d}_L^i - d_L(z, H_0)}{ \sqrt{2} \sigma_{d_L}}  \right ) \right ].
\end{align}
where erf is the unilateral error function of the standard normal distribution. The uncertainty $\sigma_{d_L}$ is taken to be a constant fraction of $d_L$, such that
$\sigma_{d_L} / d_L = C < 1$.

The galaxy catalog information is used to compute the population model prior $\ppop(\vec{\theta} |
	\vec{\lambda}) = \ppop(z | H_0)$. Each galaxy in the catalog contributes with a term

\begin{equation}
	\mathcal{L}_\text{EM}(\hat{z}_i | z_i)p(z_i | H_0),
\end{equation}
where $z_i$ and $\hat{z}_i$ are the galaxy's true and measured values, respectively. The likelihood encodes the measurement uncertainty, while the redshift prior
depends on our knowledge of the galaxy distribution on redshift. A simple choice is to pick $p(z_i | H_0)$ to be uniform in a comoving volume, $p(z_i | H_0) \propto dV_c /dz$. The posterior on $H_0$ becomes
\begin{equation}
	\given{H_0}{d_\text{EM}, d_\text{GW}} \propto
	\alpha^{-1}(H_0) \left [ \sum_{i=1}^{N_\text{gal}} \mathcal{L}_\text{GW}(d_\text{GW} | d_L(z_i, H_0)) \right ]
	\prod_{j=1}^{N_\text{gal}} \mathcal{L}_\text{EM}(\hat{z}_j | z_j)p(z_j | H_0).
\end{equation}

Note that in the above expression we are implicitly neglecting cross-correlations between galaxies,
for instance due to clustering. On the approximation that the galaxy redshifts are measured
perfectly, the likelihood $\mathcal{L}_\text{EM}$ becomes a delta function, and the posterior for a
single GW event reduces to a simple form:

\begin{equation}
	\label{eq:stat:perfect-redshift-posterior}
	\given{H_0}{d_\text{EM}, d_\text{GW}} \propto
	\frac{\sum_{i=1}^{N_\text{gal}} \mathcal{L}_\text{GW}(d^\text{GW}_L | d_L(\hat{z}_i, H_0))}{\sum_{i=1}^{N_\text{gal}} \pdet(\hat{z}_i, H_0)}.
\end{equation}

Alternatively, we model the photo-$z$ redshift likelihood as a gaussian,

\begin{equation}
	\mathcal{L}_\text{EM}(\hat{z}_j | z_j) = \frac{1}{\sqrt{2 \pi} \sigma_z} \exp{\left [-\frac{1}{2} \left (\frac{\hat{z}_i - z_i}{\sigma_z} \right )^2 \right ]},
\end{equation}

with $\sigma_z \sim \rm{min}\{0.013(1+z)^3, 0.015\}$, following an empirical fit described in
Ref.~\onlinecite{DES:2019ccw}.

\bibliography{refs}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file apssamp.tex ******
